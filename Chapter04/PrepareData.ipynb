{
 "cells": [
  {
   "source": [
    "# Chapter 3: Data preparation at scale using Amazon SageMaker Data Wrangler and Amazon SageMaker Processing\n",
    "\n",
    "In this notebook we'll perform the following steps:\n",
    "\n",
    "* Create a table in the Glue catalog for our data steps\n",
    "* Run a SageMaker Processing job to prepare the full data set\n",
    "\n",
    "You need to define the following variables:\n",
    "\n",
    "* `s3_bucket`: Bucket with the data set\n",
    "* `glue_db_name`: Glue database name\n",
    "* `glue_tbl_name`: Glue table name\n",
    "* `s3_prefix_parquet`: Location of the Parquet tables in the S3 bucket\n",
    "* `s3_output_prefix`: Location to store the prepared data in the S3 bucket\n",
    "* `s3_prefix`: Location of the JSON data in the S3 bucket\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Glue Catalog"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'MyBucket'\n",
    "glue_db_name = 'MyDatabase'\n",
    "glue_tbl_name = 'openaq'\n",
    "s3_prefix = 'openaq/realtime'\n",
    "s3_prefix_parquet = 'openaq/realtime-parquet-gzipped/tables'\n",
    "s3_output_prefix = 'prepared'\n",
    "\n",
    "import boto3\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue = boto3.client('glue')\n",
    "response = glue.create_database(\n",
    "    DatabaseInput={\n",
    "        'Name': glue_db_name,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue.create_table(\n",
    "    DatabaseName=glue_db_name,\n",
    "    TableInput={\n",
    "        'Name': glue_tbl_name,\n",
    "        'StorageDescriptor': {\n",
    "            'Columns': [\n",
    "                {\n",
    "                    \"Name\": \"date\",\n",
    "                    \"Type\": \"struct<utc:string,local:string>\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"parameter\",\n",
    "                    \"Type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"location\",\n",
    "                    \"Type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"value\",\n",
    "                    \"Type\": \"double\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"unit\",\n",
    "                    \"Type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"city\",\n",
    "                    \"Type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"attribution\",\n",
    "                    \"Type\": \"array<struct<name:string,url:string>>\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"averagingperiod\",\n",
    "                    \"Type\": \"struct<value:double,unit:string>\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"coordinates\",\n",
    "                    \"Type\": \"struct<latitude:double,longitude:double>\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"country\",\n",
    "                    \"Type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"sourcename\",\n",
    "                    \"Type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"sourcetype\",\n",
    "                    \"Type\": \"string\"\n",
    "                },\n",
    "                {\n",
    "                    \"Name\": \"mobile\",\n",
    "                    \"Type\": \"boolean\"\n",
    "                }\n",
    "            ],\n",
    "            'Location': 's3://' + s3_bucket + '/' + s3_prefix + '/',\n",
    "            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
    "            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "            'Compressed': False,\n",
    "            'SerdeInfo': {\n",
    "                'SerializationLibrary': 'org.openx.data.jsonserde.JsonSerDe',\n",
    "                \"Parameters\": {\n",
    "                    \"paths\": \"attribution,averagingPeriod,city,coordinates,country,date,location,mobile,parameter,sourceName,sourceType,unit,value\"\n",
    "                }\n",
    "            },\n",
    "            'Parameters': {\n",
    "                \"classification\": \"json\",\n",
    "                \"compressionType\": \"none\",\n",
    "            },\n",
    "            'StoredAsSubDirectories': False,\n",
    "        },\n",
    "        'PartitionKeys': [\n",
    "            {\n",
    "                \"Name\": \"aggdate\",\n",
    "                \"Type\": \"string\"\n",
    "            },\n",
    "        ],\n",
    "        'TableType': 'EXTERNAL_TABLE',\n",
    "        'Parameters': {\n",
    "            \"classification\": \"json\",\n",
    "            \"compressionType\": \"none\",\n",
    "        }\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_to_add = []\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket=s3_bucket,\n",
    "    Prefix=s3_prefix + '/'\n",
    ")\n",
    "for r in response['Contents']:\n",
    "    partitions_to_add.append(r['Key'])\n",
    "while response['IsTruncated']:\n",
    "    token = response['NextContinuationToken']\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=s3_bucket,\n",
    "        Prefix=s3_prefix,\n",
    "        ContinuationToken=token\n",
    "    ) \n",
    "    for r in response['Contents']:\n",
    "        partitions_to_add.append(r['Key'])\n",
    "    if response['IsTruncated']:\n",
    "        oken = response['NextContinuationToken']\n",
    "    print(\"Getting next batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Need to add {len(partitions_to_add)} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_def(p):\n",
    "    part_value = p.split('/')[-2]\n",
    "    return {\n",
    "                'Values': [\n",
    "                    part_value\n",
    "                ],\n",
    "                'StorageDescriptor': {\n",
    "                    'Columns': [\n",
    "                        {\n",
    "                            \"Name\": \"date\",\n",
    "                            \"Type\": \"struct<utc:string,local:string>\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"parameter\",\n",
    "                            \"Type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"location\",\n",
    "                            \"Type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"value\",\n",
    "                            \"Type\": \"double\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"unit\",\n",
    "                            \"Type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"city\",\n",
    "                            \"Type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"attribution\",\n",
    "                            \"Type\": \"array<struct<name:string,url:string>>\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"averagingperiod\",\n",
    "                            \"Type\": \"struct<value:double,unit:string>\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"coordinates\",\n",
    "                            \"Type\": \"struct<latitude:double,longitude:double>\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"country\",\n",
    "                            \"Type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"sourcename\",\n",
    "                            \"Type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"sourcetype\",\n",
    "                            \"Type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"Name\": \"mobile\",\n",
    "                            \"Type\": \"boolean\"\n",
    "                        }\n",
    "                    ],\n",
    "                    'Location': f\"s3://{s3_bucket}/{s3_prefix}/{part_value}/\",\n",
    "                    'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
    "                    'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "                    'Compressed': False,\n",
    "                    'SerdeInfo': {\n",
    "                        'SerializationLibrary': 'org.openx.data.jsonserde.JsonSerDe',\n",
    "                        \"Parameters\": {\n",
    "                            \"paths\": \"attribution,averagingPeriod,city,coordinates,country,date,location,mobile,parameter,sourceName,sourceType,unit,value\"\n",
    "                        }\n",
    "                    },\n",
    "                    'StoredAsSubDirectories': False\n",
    "                },\n",
    "                'Parameters': {\n",
    "                    \"classification\": \"json\",\n",
    "                    \"compressionType\": \"none\",\n",
    "                },\n",
    "\n",
    "\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in chunks(partitions_to_add, 100):\n",
    "    response = glue.batch_create_partition(\n",
    "        DatabaseName=glue_db_name,\n",
    "        TableName=glue_tbl_name,\n",
    "        PartitionInputList=[get_part_def(p) for p in batch]\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Processing Job"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-preprocessor\",\n",
    "    framework_version=\"3.0\",\n",
    "    role=role,\n",
    "    instance_count=15,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "configuration = [\n",
    "    {\n",
    "    \"Classification\": \"spark-defaults\",\n",
    "    \"Properties\": {\"spark.executor.memory\": \"18g\", \n",
    "                   \"spark.yarn.executor.memoryOverhead\": \"3g\",\n",
    "                   \"spark.driver.memory\": \"18g\",\n",
    "                   \"spark.yarn.driver.memoryOverhead\": \"3g\",\n",
    "                   \"spark.executor.cores\": \"5\", \n",
    "                   \"spark.driver.cores\": \"5\",\n",
    "                   \"spark.executor.instances\": \"44\",\n",
    "                   \"spark.default.parallelism\": \"440\",\n",
    "                   \"spark.dynamicAllocation.enabled\": \"false\"\n",
    "                  },\n",
    "    },\n",
    "    {\n",
    "    \"Classification\": \"yarn-site\",\n",
    "    \"Properties\": {\"yarn.nodemanager.vmem-check-enabled\": \"false\", \n",
    "                   \"yarn.nodemanager.mmem-check-enabled\": \"false\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"scripts/preprocess.py\",\n",
    "    submit_jars=[\"s3://crawler-public/json/serde/json-serde.jar\"],\n",
    "    arguments=['--s3_input_bucket', s3_bucket,\n",
    "               '--s3_input_key_prefix', s3_prefix_parquet,\n",
    "               '--s3_output_bucket', s3_bucket,\n",
    "               '--s3_output_key_prefix', s3_output_prefix],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(s3_bucket, 'sparklogs'),\n",
    "    logs=True,\n",
    "    configuration=configuration\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}