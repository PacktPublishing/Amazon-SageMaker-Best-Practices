{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Optimize Model Hosting and Inference Costs\n",
    "\n",
    "In this chapter we'll explore different techniques for optimizing model hosting and inference costs and performance.\n",
    "\n",
    "In order to complete this chapter, you need to fill out the following inputs:\n",
    "\n",
    "* `s3_bucket`: The S3 bucket containing your data\n",
    "* `s3_prefix`: The folder in the S3 bucket containing the prepared data set\n",
    "* `s3_prefix_parquet`: The location of the Parquet tables in S3\n",
    "* `s3_output_prefix`: The location for new data output in S3\n",
    "* `region`: The AWS region you're working in\n",
    "* `m_prefix`: The folder in the S3 bucket to store temporary models and output\n",
    "* `test_file_name`: The name of a file in the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'MyBucket'\n",
    "s3_prefix = 'prepared'\n",
    "s3_prefix_parquet = 'openaq/realtime-parquet-gzipped/tables'\n",
    "s3_output_prefix = 'prepared_param'\n",
    "region = 'us-east-1'\n",
    "m_prefix = 'xgboost-sample'\n",
    "test_file_name = 'part-0000.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time and Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"5\"}\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "\n",
    "output_path = 's3://{}/{}/{}/output'.format(s3_bucket, m_prefix, 'xgboost')\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.12xlarge', \n",
    "                                          volume_size=200, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/\".format(s3_bucket, s3_prefix, 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/\".format(s3_bucket, s3_prefix, 'validation'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = \"s3://{}/{}/{}/\".format(s3_bucket, s3_prefix, 'test')\n",
    "batch_output = \"s3://{}/{}/{}/\".format(s3_bucket, \"xgboost-sample\", 'xform')\n",
    "transformer = estimator.transformer(instance_count=1, instance_type='ml.m5.4xlarge', output_path=batch_output, max_payload=3)\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type=content_type, split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                            instance_type='ml.m5.2xlarge',\n",
    "                            serializer=CSVSerializer(),\n",
    "                            deserializer=JSONDeserializer()\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(s3_bucket, f\"{s3_prefix}/test/{test_file_name}\", 't_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('t_file.csv', 'r') as TF:\n",
    "    t_lines = TF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tl in t_lines[0:5]:\n",
    "    result = predictor.predict(tl.strip())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing\n",
    "\n",
    "In this section we'll run two versions of the same model in an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_v2 = {\n",
    "        \"max_depth\":\"10\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"5\"}\n",
    "\n",
    "estimator_v2 = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.12xlarge', \n",
    "                                          volume_size=200, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "predictor_v2 = estimator_v2.deploy(initial_instance_count=1,\n",
    "                            instance_type='ml.m5.2xlarge',\n",
    "                            serializer=CSVSerializer(),\n",
    "                            deserializer=JSONDeserializer()\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = predictor._model_names[0]\n",
    "model2 = predictor_v2._model_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import production_variant\n",
    "\n",
    "variant1 = production_variant(model_name=model1,\n",
    "                              instance_type=\"ml.m5.xlarge\",\n",
    "                              initial_instance_count=1,\n",
    "                              variant_name='Variant1',\n",
    "                              initial_weight=1)\n",
    "variant2 = production_variant(model_name=model2,\n",
    "                              instance_type=\"ml.m5.xlarge\",\n",
    "                              initial_instance_count=1,\n",
    "                              variant_name='Variant2',\n",
    "                              initial_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "smsession = Session()\n",
    "\n",
    "smsession.endpoint_from_production_variants(\n",
    "    name='mmendpoint',\n",
    "    production_variants=[variant1, variant2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "import boto3\n",
    "from botocore.response import StreamingBody\n",
    "\n",
    "smrt = boto3.Session().client(\"sagemaker-runtime\")\n",
    "\n",
    "for tl in t_lines[0:50]:\n",
    "    result = smrt.invoke_endpoint(EndpointName='mmendpoint',\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=tl.strip())\n",
    "    rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n",
    "    print(f\"Result from {result['InvokedProductionVariant']} = {rbody.read().decode('utf-8')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple models in a single endpoint\n",
    "\n",
    "In this section we'll create an endpoint that serves traffic for different air quality parameters using different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-preprocessor\",\n",
    "    framework_version=\"3.0\",\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=15,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    max_runtime_in_seconds=7200,\n",
    ")\n",
    "\n",
    "configuration = [\n",
    "    {\n",
    "    \"Classification\": \"spark-defaults\",\n",
    "    \"Properties\": {\"spark.executor.memory\": \"18g\", \n",
    "                   \"spark.yarn.executor.memoryOverhead\": \"3g\",\n",
    "                   \"spark.driver.memory\": \"18g\",\n",
    "                   \"spark.yarn.driver.memoryOverhead\": \"3g\",\n",
    "                   \"spark.executor.cores\": \"5\", \n",
    "                   \"spark.driver.cores\": \"5\",\n",
    "                   \"spark.executor.instances\": \"44\",\n",
    "                   \"spark.default.parallelism\": \"440\",\n",
    "                   \"spark.dynamicAllocation.enabled\": \"false\"\n",
    "                  },\n",
    "    },\n",
    "    {\n",
    "    \"Classification\": \"yarn-site\",\n",
    "    \"Properties\": {\"yarn.nodemanager.vmem-check-enabled\": \"false\", \n",
    "                   \"yarn.nodemanager.mmem-check-enabled\": \"false\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"scripts/preprocess_param.py\",\n",
    "    submit_jars=[\"s3://crawler-public/json/serde/json-serde.jar\"],\n",
    "    arguments=['--s3_input_bucket', s3_bucket,\n",
    "               '--s3_input_key_prefix', s3_prefix_parquet,\n",
    "               '--s3_output_bucket', s3_bucket,\n",
    "               '--s3_output_key_prefix', f\"{s3_output_prefix}/pm25\",\n",
    "               '--parameter', 'pm25',],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(s3_bucket, 'sparklogs'),\n",
    "    logs=True,\n",
    "    configuration=configuration\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"scripts/preprocess_param.py\",\n",
    "    submit_jars=[\"s3://crawler-public/json/serde/json-serde.jar\"],\n",
    "    arguments=['--s3_input_bucket', s3_bucket,\n",
    "               '--s3_input_key_prefix', s3_prefix_parquet,\n",
    "               '--s3_output_bucket', s3_bucket,\n",
    "               '--s3_output_key_prefix', f\"{s3_output_prefix}/o3\",\n",
    "               '--parameter', 'o3',],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(s3_bucket, 'sparklogs'),\n",
    "    logs=True,\n",
    "    configuration=configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 's3://{}/{}/{}/output'.format(s3_bucket, m_prefix, 'o3')\n",
    "\n",
    "estimator_o3 = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.12xlarge', \n",
    "                                          volume_size=200,  \n",
    "                                          output_path=output_path)\n",
    "\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/{}/\".format(s3_bucket, s3_output_prefix, 'o3', 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/{}/\".format(s3_bucket, s3_output_prefix, 'o3', 'validation'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator_o3.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 's3://{}/{}/{}/output'.format(s3_bucket, m_prefix, 'pm25')\n",
    "\n",
    "estimator_pm25 = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.12xlarge', \n",
    "                                          volume_size=200, \n",
    "                                          output_path=output_path)\n",
    "\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/{}/\".format(s3_bucket, s3_output_prefix, 'pm25', 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/{}/\".format(s3_bucket, s3_output_prefix, 'pm25', 'validation'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator_pm25.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = estimator_o3.create_model(role=sagemaker.get_execution_role(), image_uri=xgboost_container)\n",
    "\n",
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "model_data_prefix = f's3://{s3_bucket}/{m_prefix}/mma/'\n",
    "\n",
    "model_name = 'xgboost-mma'\n",
    "mme = MultiDataModel(name=model_name,\n",
    "                     model_data_prefix=model_data_prefix,\n",
    "                     model=model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mme.deploy(initial_instance_count=1,\n",
    "                       instance_type='ml.m5.2xlarge',\n",
    "                       endpoint_name=model_name,\n",
    "                      serializer=CSVSerializer(),\n",
    "                           deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in [estimator_o3, estimator_pm25]:\n",
    "    artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "    #print(artifact_path)\n",
    "    m_name = artifact_path.split('/')[4]+'.tar.gz'\n",
    "    #print(m_name)\n",
    "    # This is copying over the model artifact to the S3 location for the MME.\n",
    "    mme.add_model(model_data_source=artifact_path, model_data_path=m_name)\n",
    "    \n",
    "list(mme.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(s3_bucket, f\"{s3_output_prefix}/pm25/test/part-00120-81a51ddd-c8b5-47d0-9431-0a5da6158754-c000.csv\", 'pm25.csv')\n",
    "s3.download_file(s3_bucket, f\"{s3_output_prefix}/o3/test/part-00214-ae1a5b74-e187-4b62-ae4a-385afcbaa766-c000.csv\", 'o3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pm25.csv', 'r') as TF:\n",
    "    pm_lines = TF.readlines()\n",
    "with open('o3.csv', 'r') as TF:\n",
    "    o_lines = TF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tl in pm_lines[0:5]:\n",
    "    result = predictor.predict(data = tl.strip(), target_model='pm25.tar.gz')\n",
    "    print(result)\n",
    "\n",
    "for tl in o_lines[0:5]:\n",
    "    result = predictor.predict(data = tl.strip(), target_model='o3.tar.gz')\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Inference\n",
    "\n",
    "In this section we'll add elastic inference capacity to an existing endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_ei = predictor.deploy(initial_instance_count = 1, instance_type = 'ml.m5.xlarge', \n",
    "                                 serializer=CSVSerializer(),\n",
    "                                deserializer=JSONDeserializer(),\n",
    "                                   accelerator_type='ml.eia2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model optimization with SageMaker Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = len(t_lines[0].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "\n",
    "n_prefix = 'xgboost-sample-neo'\n",
    "n_output_path = 's3://{}/{}/{}/output'.format(s3_bucket, n_prefix, 'xgboost-neo')\n",
    "\n",
    "m1 = Model(xgboost_container, \n",
    "           model_data=estimator.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts'], \n",
    "           role=sagemaker.get_execution_role())\n",
    "neo_model = m1.compile('ml_m5', \n",
    "           {'data':[1, ncols]}, \n",
    "           n_output_path, \n",
    "           sagemaker.get_execution_role(), \n",
    "           framework='xgboost', \n",
    "           framework_version='latest',\n",
    "           job_name = 'neojob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_predictor = neo_model.deploy(initial_instance_count = 1, instance_type = 'ml.m5.xlarge', \n",
    "                                 serializer=CSVSerializer(),\n",
    "                                deserializer=JSONDeserializer(),\n",
    "                                endpoint_name='neo_endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tl in t_lines[0:5]:\n",
    "    result = smrt.invoke_endpoint(EndpointName='neo_endpoint',\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=tl.strip())\n",
    "    rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n",
    "    print(f\"Result from {result['InvokedProductionVariant']} = {rbody.read().decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
