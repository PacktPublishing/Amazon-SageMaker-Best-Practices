{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quality drift monitoring with Amazon SageMaker Model Monitor\n",
    "\n",
    "\n",
    "This notebook provides a walkthrough of the high level steps involved in monitoring a production ML model with SageMaker Model Monitor for data drift. To demonstrate the data drift monitoring we will use a pre-trained model to deploy an endpoint.  We provide the pre-trained model artifact along with baseline and test datasets along with this notebook.\n",
    "\n",
    "1. Set up\n",
    "2. Enable datacapture on a SageMaker endpoint \n",
    "3. Generate a baseline with Model Monitor \n",
    "4. Schedule continous monitoring\n",
    "5. Analyze monitoring results\n",
    "6. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from botocore.response import StreamingBody\n",
    "\n",
    "from sagemaker import get_execution_role, session, Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from threading import Thread\n",
    "\n",
    "from time import sleep\n",
    "from time import gmtime, strftime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoleArn: arn:aws:iam::802439482869:role/service-role/AmazonSageMaker-ExecutionRole-20210418T143524\n",
      "Capture path: s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/datacapture\n",
      "Report path: s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/reports\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "#This is the bucket into which the data is captured\n",
    "#bucket = 'bestpractices-bucket-sm'\n",
    "bucket = 'datascience-environment-notebookinstance--06dc7a0224df'\n",
    "prefix = \"ModelQualityMonitor\"\n",
    "\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "code_prefix = \"{}/code\".format(prefix)\n",
    "#s3_code_preprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"preprocessor.py\")\n",
    "#s3_code_postprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"postprocessor.py\")\n",
    "\n",
    "\n",
    "ground_truth_upload_path = (\n",
    "    f\"s3://{bucket}/{prefix}/ground_truth_data/{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    ")\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "#print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "#print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Setup service clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "sagemaker_runtime_client = boto3.Session().client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Enable datacapture on a SageMaker endpoint \n",
    "\n",
    "Create an endpoint to showcase the data capture capability in action.\n",
    "\n",
    "For the endpoint we will use a pre-trained XGBoost model that is ready to deploy. This model was trained in the previous chapters using the weather dataset and has been included in the model directory for ease of use.\n",
    "\n",
    "Note that you can also train a new model and use your model and data below as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Upload the model object into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open(\"model/weather-prediction-model.tar.gz\", \"rb\")\n",
    "s3_key = os.path.join(prefix, \"weather-prediction-model.tar.gz\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  Create SageMaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://datascience-environment-notebookinstance--06dc7a0224df.s3-us-west-2.amazonaws.com/ModelQualityMonitor/weather-prediction-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_url = \"https://{}.s3-{}.amazonaws.com/{}/weather-prediction-model.tar.gz\".format(\n",
    "    bucket, region, prefix\n",
    ")\n",
    "\n",
    "print(model_url)\n",
    "\n",
    "image_uri = retrieve(\"xgboost\", boto3.Session().region_name, \"1.2-1\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3  Configure datacapture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture on the endpoint, you specify the new capture option called `DataCaptureConfig`. On enabling data capture, input to and output from the SageMaker endpoint are captured and saved in S3. Input captured includes the live inference traffic requests and output captured includes predictions from the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName=xgb-weather-prediction-model-monitor-2021-08-03-06-29-41\n",
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "endpoint_name = \"xgb-weather-prediction-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Capture data from endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for about 3 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Download the test files to execute inferences\n",
    "#s3 = boto3.client('s3')\n",
    "#s3_prefix = 'prepared'\n",
    "#test_file_name='part-00000-0b01100d-c57d-4375-9fa3-e11879c4cd0a-c000.csv'\n",
    "#s3.download_file(bucket, f\"{s3_prefix}/test/{test_file_name}\", 't_file.csv')\n",
    "\n",
    "#with open('t_file.csv', 'r') as TF:\n",
    " #   t_lines = TF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use the test file in the data directory  to execute inferences using the test file 't_file.csv' provided\n",
    "with open('data/t_file.csv', 'r') as TF:\n",
    "    t_lines = TF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a method to run inferences against the endpoint\n",
    "def get_predictions():\n",
    "    smrt = boto3.Session().client(\"sagemaker-runtime\")\n",
    "    #Skip the first line since it has column headers\n",
    "    for tl in t_lines[1:50]:\n",
    "        #Remove the first column since it is the label\n",
    "        test_list = tl.split(\",\")\n",
    "        test_list.pop(0)\n",
    "        test_string = ','.join([str(elem) for elem in test_list])\n",
    "        \n",
    "        #print(\"invoking with payload \" + test_string)\n",
    "    \n",
    "        result = smrt.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=test_string)\n",
    "        #print(result)                              \n",
    "        rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n",
    "        #print(f\"Result from {result['InvokedProductionVariant']} = {rbody.read().decode('utf-8')}\")\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................."
     ]
    }
   ],
   "source": [
    "#Get predictions\n",
    "get_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5  View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/datacapture'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_capture_upload_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "ModelQualityMonitor/datacapture/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41/AllTraffic/2021/08/03/06/37-13-691-105c7f5d-682e-4405-ba5a-0c38e213f99a.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Note : If you see an error in this cell, it could be because the captured files didn't appear in S3 yet.\n",
    "#Retry after a minute.\n",
    "\n",
    "s3_client = boto3.Session().client(\"s3\")\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "\n",
    "#print(result.get(\"Contents\"))\n",
    "\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the contents of a single capture file. Here you should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"747a755b-8410-408e-86f2-1a76988d346e\",\"inferenceTime\":\"2021-08-03T06:37:13Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"d631cfc1-bb7b-4a19-bdd3-5ed8f2f3bf9b\",\"inferenceTime\":\"2021-08-03T06:37:14Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"6a06342d-0c7c-4900-a270-367702618549\",\"inferenceTime\":\"2021-08-03T06:37:14Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"00d3f860-293c-4d90-b301-d1269d2a0889\",\"inferenceTime\":\"2021-08-03T06:37:15Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetad\n"
     ]
    }
   ],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file so that you can observe a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"-4.902510643005371\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"747a755b-8410-408e-86f2-1a76988d346e\",\n",
      "    \"inferenceTime\": \"2021-08-03T06:37:13Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In the example, you provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, you expose the encoding that you used to encode the input and output payloads in the capture format with the `encoding` value.\n",
    "\n",
    "To recap, you observed how you can enable capturing the input or output payloads to an endpoint with a new parameter. You have also observed what the captured format looks like in Amazon S3. Next, continue to explore how Amazon SageMaker helps with monitoring the data collected in Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a baseline with Model Monitor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, Amazon SageMaker provides the capability for you to monitor and evaluate the data observed by the endpoints. To see this in action, lets first create a baseline with will then be used to compare the realtime traffic against. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Setup the baseline dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating a baseline, you need to provide a baseline dataset.  \n",
    "\n",
    "The model quality baseline job compares the labels in a baseline data set with \n",
    "the predictions made by the model. So instead of using the training data directly, \n",
    "you have to first generate a baseline dataset consisting of labels by running predictions \n",
    "against the model. In this example, we will use the validation dataset to run predictions against the model and use the results as input to the baseline generation job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use the validation file in the data directory  to execute inferences using the test file 'validation_file.csv' provided\n",
    "with open('data/validation_data.csv', 'r') as TF:\n",
    "    v_lines = TF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline_file = 'model-quality-baseline-data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "with open(f\"data/{model_baseline_file}\", \"w\") as baseline_file:\n",
    "    baseline_file.write(\"probability,prediction,label\\n\")  # Header of the file\n",
    "    for vl in v_lines[1:300]:\n",
    "        #Remove the first column since it is the label\n",
    "        validation_list = vl.split(\",\")\n",
    "        label = validation_list.pop(0)\n",
    "        validation_string = ','.join([str(elem) for elem in validation_list])\n",
    "        \n",
    "        #print(\"invoking with payload \" + test_string)\n",
    "    \n",
    "        result = sagemaker_runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=validation_string)\n",
    "        #print(result)                              \n",
    "        rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n",
    "        #print(rbody)\n",
    "        prediction = rbody.read().decode('utf-8')\n",
    "        #print('prediction : ' , prediction)\n",
    "        ##Using prediction as the probability\n",
    "        baseline_file.write(f\"{prediction},{prediction},{label}\\n\")\n",
    "        #print(f\"label {label} ; prediction {prediction} \")\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability,prediction,label\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n",
      "-4.902510643005371,-4.902510643005371,-7.535634515882377\r\n"
     ]
    }
   ],
   "source": [
    "!head data/{model_baseline_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Upload the predictions as a baseline dataset.\n",
    "Now we will upload the predictions made using validation dataset to S3 which will be used for creating model quality baseline statistics and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/data\n",
      "Baseline results uri: s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/results\n"
     ]
    }
   ],
   "source": [
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = f\"s3://{bucket}/{baseline_data_prefix}\"\n",
    "baseline_results_uri = f\"s3://{bucket}/{baseline_results_prefix}\"\n",
    "print(f\"Baseline data uri: {baseline_data_uri}\")\n",
    "print(f\"Baseline results uri: {baseline_results_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/data/model-quality-baseline-data.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_dataset_uri = S3Uploader.upload(f\"data/{model_baseline_file}\", baseline_data_uri)\n",
    "baseline_dataset_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Create a baselining job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the training data ready in Amazon S3, start a job to `suggest` constraints. `ModelQualityMonitor.suggest_baseline(..)` starts a `ProcessingJob` using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "\n",
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the model quality baseline job\n",
    "baseline_job_name = f\"model-quality-baseline-job-{datetime.utcnow():%Y-%m-%d-%H%M}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/data/model-quality-baseline-data.csv'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_dataset_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  model-quality-baseline-job-2021-08-03-0641\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/data/model-quality-baseline-data.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".................................................................!"
     ]
    }
   ],
   "source": [
    "# Execute the baseline suggestion job.\n",
    "# You will specify problem type, in this case regression, and provide other required attributes.\n",
    "job = model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_job_name,\n",
    "    baseline_dataset=baseline_dataset_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"Regression\",\n",
    "    inference_attribute=\"prediction\",\n",
    "    probability_attribute=\"probability\",\n",
    "    ground_truth_attribute=\"label\",\n",
    ")\n",
    "job.wait(logs=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Explore the results of the baselining job\n",
    "You could see the baseline constraints and statistics files are uploaded to the S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.1 View the metrics generated\n",
    "You could see that the baseline statistics and constraints files are already uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline_job.baseline_statistics().body_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mae.value</th>\n",
       "      <td>3.955825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae.standard_deviation</th>\n",
       "      <td>0.058059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse.value</th>\n",
       "      <td>18.548753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse.standard_deviation</th>\n",
       "      <td>0.495597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse.value</th>\n",
       "      <td>4.306826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse.standard_deviation</th>\n",
       "      <td>0.058016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2.value</th>\n",
       "      <td>-7.741915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2.standard_deviation</th>\n",
       "      <td>1.346406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0\n",
       "mae.value                 3.955825\n",
       "mae.standard_deviation    0.058059\n",
       "mse.value                18.548753\n",
       "mse.standard_deviation    0.495597\n",
       "rmse.value                4.306826\n",
       "rmse.standard_deviation   0.058016\n",
       "r2.value                 -7.741915\n",
       "r2.standard_deviation     1.346406"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "binary_metrics = baseline_job.baseline_statistics().body_dict[\"regression_metrics\"]\n",
    "pd.json_normalize(binary_metrics).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.2 View the constraints generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>comparison_operator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>3.95583</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse</th>\n",
       "      <td>18.5488</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>4.30683</td>\n",
       "      <td>GreaterThanThreshold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>-7.74191</td>\n",
       "      <td>LessThanThreshold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     threshold   comparison_operator\n",
       "mae    3.95583  GreaterThanThreshold\n",
       "mse    18.5488  GreaterThanThreshold\n",
       "rmse   4.30683  GreaterThanThreshold\n",
       "r2    -7.74191     LessThanThreshold"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(baseline_job.suggested_constraints().body_dict[\"regression_constraints\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Schedule continous monitoring\n",
    "When you have collected the data above, analyze and monitor the data with Monitoring Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Generate prediction data for Model Quality  Monitoring\n",
    "\n",
    "Start generating some artificial traffic.  The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "#smrt = boto3.Session().client(\"sagemaker-runtime\")\n",
    "\n",
    "#with open(f\"data/{validate_dataset}\", \"w\") as baseline_file:\n",
    " #   baseline_file.write(\"probability,prediction,label\\n\")  # Header of the file\n",
    "i = 0\n",
    "for tl in t_lines[1:300]:\n",
    "        #Remove the first column since it is the label\n",
    "        test_list = tl.split(\",\")\n",
    "        label = test_list.pop(0)\n",
    "        test_string = ','.join([str(elem) for elem in test_list])\n",
    "        \n",
    "        #print(\"invoking with payload \" + test_string)\n",
    "    \n",
    "        result = sagemaker_runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=test_string,\n",
    "                                    InferenceId=str(i))  # unique ID per row\n",
    "        #print(result)                              \n",
    "        rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n",
    "        #print(rbody)\n",
    "        prediction = rbody.read().decode('utf-8')\n",
    "        #print('prediction : ' , prediction)\n",
    "        ##Using prediction as the probability\n",
    "        #baseline_file.write(f\"{prediction},{prediction},{label}\\n\")\n",
    "        #print(f\"label {label} ; prediction {prediction} \")\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        i += 1\n",
    "        sleep(0.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the new attribute `inferenceId`, which we're setting when invoking the endpoint. This is used to join the prediction data with the ground truth data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for captures to show up\n",
      "Found Capture Files:\n",
      "s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/datacapture/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41/AllTraffic/2021/08/03/06/41-08-476-2e02d9ae-512f-4188-9d8d-0ed1f1534950.jsonl\n",
      " s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/datacapture/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41/AllTraffic/2021/08/03/06/47-09-023-c2948643-51cc-456c-9c9c-6520614a3f77.jsonl\n",
      " s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/datacapture/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41/AllTraffic/2021/08/03/06/48-09-401-087f7348-9179-442d-a6ae-11317b831d87.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting for captures to show up\", end=\"\")\n",
    "for _ in range(120):\n",
    "    capture_files = sorted(S3Downloader.list(f\"{s3_capture_upload_path}/{endpoint_name}\"))\n",
    "    if capture_files:\n",
    "        capture_file = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")\n",
    "        capture_record = json.loads(capture_file[0])\n",
    "        if \"inferenceId\" in capture_record[\"eventMetadata\"]:\n",
    "            break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(1)\n",
    "print()\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the contents of a single capture file. Here you should see all the data captured in an Amazon SageMaker specific JSON-line formatted file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,2791.0,76.0,2.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.17314249277114868\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"73cccb5c-fb3a-4973-b55c-5d0e3c043cf6\",\"inferenceId\":\"234\",\"inferenceTime\":\"2021-08-03T06:49:08Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,4252.0,39.0,2.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"0.17314249277114868\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"96c5a119-2c78-4358-b01f-5b8fae830240\",\"inferenceId\":\"235\",\"inferenceTime\":\"2021-08-03T06:49:09Z\"},\"eventVersion\":\"0\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(capture_file[-3:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file so that you can observe a little better.\n",
    "\n",
    "Again, notice the `inferenceId` attribute that is set as part of the invoke_endpoint call.  If this is present, it will be used to join with ground truth data (otherwise `eventId` will be used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"0,2020,12,4,31,0,80.0,0.0,6.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0\\n\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"-4.3170342445373535\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"d9b99b7b-b3f9-4a4f-bd89-a710b234ea6a\",\n",
      "    \"inferenceId\": \"118\",\n",
      "    \"inferenceTime\": \"2021-08-03T06:48:09Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(capture_record, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Generate synthetic ground truth\n",
    "\n",
    "Next, start generating ground truth data. The model quality job will fail if there's no ground truth data to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def ground_truth_with_id(inference_id):\n",
    "    random.seed(inference_id)  # to get consistent results\n",
    "    rand = random.random()\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"1\" if rand < 0.7 else \"0\",  # randomly generate positive labels 70% of the time #\n",
    "             # TODO : Need to make this a decimal??\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": str(inference_id),\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "\n",
    "def upload_ground_truth(records, upload_time):\n",
    "    fake_records = [json.dumps(r) for r in records]\n",
    "    data_to_upload = \"\\n\".join(fake_records)\n",
    "    target_s3_uri = f\"{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    print(f\"Uploading {len(fake_records)} records to\", target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 300 records to s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/ground_truth_data/2021-08-03-06-29-40/2021/08/03/06/4942.jsonl\n"
     ]
    }
   ],
   "source": [
    "#NUM_GROUND_TRUTH_RECORDS = 334  # 334 are the number of rows in data we're sending for inference\n",
    "NUM_GROUND_TRUTH_RECORDS = 300\n",
    "\n",
    "\n",
    "def generate_fake_ground_truth_forever():\n",
    "    j = 0\n",
    "    while True:\n",
    "        fake_records = [ground_truth_with_id(i) for i in range(NUM_GROUND_TRUTH_RECORDS)]\n",
    "        upload_ground_truth(fake_records, datetime.utcnow())\n",
    "        j = (j + 1) % 5\n",
    "        sleep(60 * 60)  # do this once an hour\n",
    "\n",
    "\n",
    "gt_thread = Thread(target=generate_fake_ground_truth_forever)\n",
    "gt_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Create a monitoring schedule\n",
    "\n",
    "Now that you have the baseline information and ground truth labels, create a monitoring schedule to run model quality monitoring job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Monitoring schedule name\n",
    "model_quality_monitor_schedule_name = (\n",
    "    f\"model-quality-monitoring-schedule-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the monitoring schedule you need to specify how to interpret an endpoint's output. Given that the endpoint in this notebook outputs CSV data, the below code specifies that the first column of the output, `0`, contains a probability (of churn in this example). You will further specify `0.5` as the cutoff  used to determine a positive label (that is, predict that a customer will churn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an enpointInput\n",
    "endpointInput = EndpointInput(\n",
    "    endpoint_name=endpoint_name,\n",
    "    inference_attribute='0',\n",
    "    #probability_attribute=\"0\",\n",
    "    #probability_threshold_attribute=0.5,\n",
    "    destination=\"/opt/ml/processing/input_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the monitoring schedule to execute every hour.\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "response = model_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=model_quality_monitor_schedule_name,\n",
    "    endpoint_input=endpointInput,\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    problem_type=\"Regression\",\n",
    "    ground_truth_input=ground_truth_upload_path,\n",
    "    constraints=baseline_job.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MonitoringScheduleArn': 'arn:aws:sagemaker:us-west-2:802439482869:monitoring-schedule/model-quality-monitoring-schedule-2021-08-03-0649',\n",
       " 'MonitoringScheduleName': 'model-quality-monitoring-schedule-2021-08-03-0649',\n",
       " 'MonitoringScheduleStatus': 'Pending',\n",
       " 'MonitoringType': 'ModelQuality',\n",
       " 'CreationTime': datetime.datetime(2021, 8, 3, 6, 49, 43, 236000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 8, 3, 6, 49, 43, 259000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleConfig': {'ScheduleConfig': {'ScheduleExpression': 'cron(0 * ? * * *)'},\n",
       "  'MonitoringJobDefinitionName': 'model-quality-job-definition-2021-08-03-06-49-42-976',\n",
       "  'MonitoringType': 'ModelQuality'},\n",
       " 'EndpointName': 'xgb-weather-prediction-model-monitor-2021-08-03-06-29-41',\n",
       " 'ResponseMetadata': {'RequestId': '67097672-ccd3-480e-98a2-c0f1d77e2ce0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '67097672-ccd3-480e-98a2-c0f1d77e2ce0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '632',\n",
       "   'date': 'Tue, 03 Aug 2021 06:49:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the monitoring schedule\n",
    "# You will see the monitoring schedule in the 'Scheduled' status\n",
    "model_quality_monitor.describe_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.5 Examine monitoring schedule executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: model-quality-monitoring-schedule-2021-08-03-0649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initially there will be no executions since the first execution happens at the top of the hour\n",
    "# Note that it is common for the execution to luanch upto 20 min after the hour.\n",
    "executions = model_quality_monitor.list_executions()\n",
    "executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for first execution............................................................................\n",
      "Execution found!\n"
     ]
    }
   ],
   "source": [
    "# Wait for the first execution of the monitoring_schedule\n",
    "print(\"Waiting for first execution\", end=\"\")\n",
    "while True:\n",
    "    execution = model_quality_monitor.describe_schedule().get(\n",
    "        \"LastMonitoringExecutionSummary\"\n",
    "    )\n",
    "    if execution:\n",
    "        break\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    sleep(10)\n",
    "print()\n",
    "print(\"Execution found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'groundtruth_input_1',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/ground_truth_data/2021-08-03-06-29-40/2021/08/03/06',\n",
       "    'LocalPath': '/opt/ml/processing/groundtruth/2021/08/03/06',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'endpoint_input_1',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/datacapture/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41/AllTraffic/2021/08/03/06',\n",
       "    'LocalPath': '/opt/ml/processing/input_data/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41/AllTraffic/2021/08/03/06',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'result',\n",
       "    'S3Output': {'S3Uri': 's3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/results/merge',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'groundtruth-merge-202108030700-259fee1f24dfb6ac09b389dd',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1,\n",
       "   'InstanceType': 'ml.m5.xlarge',\n",
       "   'VolumeSizeInGB': 20}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 1800},\n",
       " 'AppSpecification': {'ImageUri': '159807026194.dkr.ecr.us-west-2.amazonaws.com/sagemaker-model-monitor-groundtruth-merger'},\n",
       " 'Environment': {'dataset_source': '/opt/ml/processing/input_data',\n",
       "  'ground_truth_source': '/opt/ml/processing/groundtruth',\n",
       "  'output_path': '/opt/ml/processing/output'},\n",
       " 'RoleArn': 'arn:aws:iam::802439482869:role/service-role/AmazonSageMaker-ExecutionRole-20210418T143524',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:802439482869:processing-job/groundtruth-merge-202108030700-259fee1f24dfb6ac09b389dd',\n",
       " 'ProcessingJobStatus': 'InProgress',\n",
       " 'LastModifiedTime': datetime.datetime(2021, 8, 3, 7, 2, 27, 823000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2021, 8, 3, 7, 2, 26, 946000, tzinfo=tzlocal()),\n",
       " 'MonitoringScheduleArn': 'arn:aws:sagemaker:us-west-2:802439482869:monitoring-schedule/model-quality-monitoring-schedule-2021-08-03-0649',\n",
       " 'ResponseMetadata': {'RequestId': '5e35d8d2-1bcd-4ff9-82e1-9de1adb87ccf',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5e35d8d2-1bcd-4ff9-82e1-9de1adb87ccf',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2166',\n",
       "   'date': 'Tue, 03 Aug 2021 07:02:39 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while not executions:\n",
    "    executions = model_quality_monitor.list_executions()\n",
    "    sleep(10)\n",
    "latest_execution = executions[-1]\n",
    "latest_execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect a specific execution (latest execution)\n",
    "In the previous cell, you picked up the latest completed or failed scheduled execution. Here are the possible terminal states and what each of them mean: \n",
    "* Completed - This means the monitoring execution completed and no issues were found in the violations report.\n",
    "* CompletedWithViolations - This means the execution completed, but constraint violations were detected.\n",
    "* Failed - The monitoring execution failed, maybe due to client error (perhaps incorrect role permissions) or infrastructure issues. Further examination of FailureReason and ExitMessage is necessary to identify what exactly happened.\n",
    "* Stopped - job exceeded max runtime or was manually stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for execution to finish...........................................................!\n",
      "groundtruth-merge-202108030700-259fee1f24dfb6ac09b389dd job status: Completed\n",
      "groundtruth-merge-202108030700-259fee1f24dfb6ac09b389dd job exit message, if any: None\n",
      "groundtruth-merge-202108030700-259fee1f24dfb6ac09b389dd job failure reason, if any: None\n",
      "Waiting for execution to finish............................................................!\n",
      "model-quality-monitoring-202108030700-259fee1f24dfb6ac09b389dd job status: Completed\n",
      "model-quality-monitoring-202108030700-259fee1f24dfb6ac09b389dd job exit message, if any: CompletedWithViolations: Job completed successfully with 1 violations.\n",
      "model-quality-monitoring-202108030700-259fee1f24dfb6ac09b389dd job failure reason, if any: None\n",
      "Execution status is: CompletedWithViolations\n",
      "{'MonitoringScheduleName': 'model-quality-monitoring-schedule-2021-08-03-0649', 'ScheduledTime': datetime.datetime(2021, 8, 3, 7, 0, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2021, 8, 3, 7, 2, 25, 538000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2021, 8, 3, 7, 13, 15, 650000, tzinfo=tzlocal()), 'MonitoringExecutionStatus': 'CompletedWithViolations', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:802439482869:processing-job/model-quality-monitoring-202108030700-259fee1f24dfb6ac09b389dd', 'EndpointName': 'xgb-weather-prediction-model-monitor-2021-08-03-06-29-41'}\n",
      "====STOP==== \n",
      " No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\n"
     ]
    }
   ],
   "source": [
    "status = execution[\"MonitoringExecutionStatus\"]\n",
    "\n",
    "while status in [\"Pending\", \"InProgress\"]:\n",
    "    print(\"Waiting for execution to finish\", end=\"\")\n",
    "    latest_execution.wait(logs=False)\n",
    "    latest_job = latest_execution.describe()\n",
    "    print()\n",
    "    print(f\"{latest_job['ProcessingJobName']} job status:\", latest_job[\"ProcessingJobStatus\"])\n",
    "    print(\n",
    "        f\"{latest_job['ProcessingJobName']} job exit message, if any:\",\n",
    "        latest_job.get(\"ExitMessage\"),\n",
    "    )\n",
    "    print(\n",
    "        f\"{latest_job['ProcessingJobName']} job failure reason, if any:\",\n",
    "        latest_job.get(\"FailureReason\"),\n",
    "    )\n",
    "    sleep(\n",
    "        30\n",
    "    )  # model quality executions consist of two Processing jobs, wait for second job to start\n",
    "    latest_execution = model_quality_monitor.list_executions()[-1]\n",
    "    execution = model_quality_monitor.describe_schedule()[\"LastMonitoringExecutionSummary\"]\n",
    "    status = execution[\"MonitoringExecutionStatus\"]\n",
    "\n",
    "print(\"Execution status is:\", status)\n",
    "\n",
    "if status != \"Completed\":\n",
    "    print(execution)\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Uri: s3://datascience-environment-notebookinstance--06dc7a0224df/ModelQualityMonitor/baselining/results/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41/model-quality-monitoring-schedule-2021-08-03-0649/2021/08/03/07\n"
     ]
    }
   ],
   "source": [
    "latest_execution = model_quality_monitor.list_executions()[-1]\n",
    "report_uri = latest_execution.describe()[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "print(\"Report Uri:\", report_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analyze monitoring results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 View violations generated by monitoring schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, they will be listed in the reports uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>constraint_check_type</th>\n",
       "      <th>description</th>\n",
       "      <th>metric_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LessThanThreshold</td>\n",
       "      <td>Metric r2 with -53.790146743270654 +/- 4.829550001308316 was LessThanThreshold '-7.7419149776821'</td>\n",
       "      <td>r2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  constraint_check_type  \\\n",
       "0     LessThanThreshold   \n",
       "\n",
       "                                                                                         description  \\\n",
       "0  Metric r2 with -53.790146743270654 +/- 4.829550001308316 was LessThanThreshold '-7.7419149776821'   \n",
       "\n",
       "  metric_name  \n",
       "0          r2  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "violations = latest_execution.constraint_violations().body_dict[\"violations\"]\n",
    "violations_df = pd.json_normalize(violations)\n",
    "violations_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that one of the violations generated is that the f2 score is less than the threshold value set as part of baselining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the violations, the monitoring schedule also emits CloudWatch metrics. In this section, you will view the metrics generated and setup an CloudWatch alarm to be triggered when the model quality drifts from the baseline thresholds. You could use CloudWatch alarms to trigger remedial actions such as retraining your model or updating the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 List the CW metrics generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CloudWatch client\n",
    "cw_client = boto3.Session().client(\"cloudwatch\")\n",
    "\n",
    "namespace = \"aws/sagemaker/Endpoints/model-metrics\"\n",
    "\n",
    "cw_dimensions = [\n",
    "    {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "    {\"Name\": \"MonitoringSchedule\", \"Value\": model_quality_monitor_schedule_name},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae\n",
      "mse\n",
      "rmse\n",
      "r2\n"
     ]
    }
   ],
   "source": [
    "# List metrics through the pagination interface\n",
    "paginator = cw_client.get_paginator(\"list_metrics\")\n",
    "\n",
    "for response in paginator.paginate(Dimensions=cw_dimensions, Namespace=namespace):\n",
    "    model_quality_metrics = response[\"Metrics\"]\n",
    "    for metric in model_quality_metrics:\n",
    "        print(metric[\"MetricName\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Create a CloudWatch Alarm\n",
    "\n",
    "Based on the cloud watch metrics, you can create a cloud watch alarm when a specific metric does not meet the threshold configured. Here you will create an alarm if the f2 value of the model fall below the threshold suggested by the baseline constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '63071239-3a34-4ab0-b4d7-022cba8fde19',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '63071239-3a34-4ab0-b4d7-022cba8fde19',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '214',\n",
       "   'date': 'Tue, 03 Aug 2021 07:13:41 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alarm_name = \"MODEL_QUALITY_R2_SCORE\"\n",
    "alarm_desc = (\n",
    "    \"Trigger an CloudWatch alarm when the r2 score drifts away from the baseline constraints\"\n",
    ")\n",
    "mdoel_quality_r2_drift_threshold = (\n",
    "    -7.7419149776821  ##Setting this threshold purposefully low to see the alarm quickly.  \n",
    ")\n",
    "metric_name = \"r2\"\n",
    "namespace = \"aws/sagemaker/Endpoints/model-metrics\"\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic=\"Average\",\n",
    "    Dimensions=[\n",
    "        {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "        {\"Name\": \"MonitoringSchedule\", \"Value\": model_quality_monitor_schedule_name},\n",
    "    ],\n",
    "    Period=600,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=mdoel_quality_r2_drift_threshold,\n",
    "    ComparisonOperator=\"LessThanOrEqualToThreshold\",\n",
    "    TreatMissingData=\"breaching\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Validation\n",
    "In a few minutes, you should see a CloudWatch alarm created. The alarm will first be in \"Insufficient Data\" state and moves into \"Alert\" state. This can be verified in the CloudWatch console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG src=images/r2_InsufficientData.png/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<IMG src=images/r2_Alarm.png/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the CW Alarm is generated, you can decide on what actions you want to take on these alerts.  A possible action could be updating the training data an retraining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other commands\n",
    "We can also start and stop the monitoring schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_quality_monitor.stop_monitoring_schedule()\n",
    "# model_quality_monitor.start_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Clean up \n",
    "\n",
    "You can keep your endpoint running to continue capturing data. If you do not plan to collect more data or use this endpoint further, you should delete the endpoint to avoid incurring additional charges. Note that deleting your endpoint does not delete the data that was captured during the model invocations. That data persists in Amazon S3 until you delete it yourself.\n",
    "\n",
    "But before that, you need to delete the schedule first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: model-quality-monitoring-schedule-2021-08-03-0649\n"
     ]
    }
   ],
   "source": [
    "model_quality_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-west-2:802439482869:endpoint/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-aa553e965610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Delete the endpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m response = sagemaker_client.delete_endpoint(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"arn:aws:sagemaker:us-west-2:802439482869:endpoint/xgb-weather-prediction-model-monitor-2021-08-03-06-29-41\"."
     ]
    }
   ],
   "source": [
    "##Delete the endpoint\n",
    "response = sagemaker_client.delete_endpoint(\n",
    "    EndpointName=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
