{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature attribute drift monitoring with Amazon SageMaker Clarify\n",
    "\n",
    "\n",
    "This notebook provides a walkthrough of the high level steps involved in monitoring a production ML model with SageMaker Clarify for bias drift and feature attribute drift. To demonstrate the data drift monitoring we will use a pre-trained model to deploy an endpoint.  We provide the pre-trained model artifact along with baseline and test datasets along with this notebook.\n",
    "\n",
    "1. Set up\n",
    "2. Enable datacapture on a SageMaker endpoint \n",
    "3. Generate a baseline with Model Monitor \n",
    "4. Schedule continous monitoring to monitor predictions for bias drift on a regular basis.\n",
    "5. Analyze bias drift monitoring results\n",
    "6. Schedule continous monitoring to monitor predictions for feature attribute drift on a regular basis.\n",
    "7. Analyze feature attribute drift monitoring results\n",
    "8. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "from botocore.response import StreamingBody\n",
    "from sagemaker import get_execution_role, session\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sagemaker import get_execution_role, image_uris, Session\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "from sagemaker.clarify import (\n",
    "    BiasConfig,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    ModelPredictedLabelConfig,\n",
    "    SHAPConfig,\n",
    ")\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_monitor import (\n",
    "    #BiasAnalysisConfig,\n",
    "    CronExpressionGenerator,\n",
    "    DataCaptureConfig,\n",
    "    EndpointInput,\n",
    "    ExplainabilityAnalysisConfig,\n",
    "    #ModelBiasMonitor,\n",
    "    ModelExplainabilityMonitor,\n",
    ")\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoleArn: arn:aws:iam::802439482869:role/service-role/AmazonSageMaker-ExecutionRole-20210418T143524\n",
      "Capture path: s3://bestpractices-bucket-sm/FeatureAttributionMonitoring/datacapture\n",
      "Report path: s3://bestpractices-bucket-sm/FeatureAttributionMonitoring/reports\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "#This is the bucket into which the data is captured\n",
    "bucket = 'bestpractices-bucket-sm' ##TODO Upadate\n",
    "prefix = \"FeatureAttributionMonitoring\"\n",
    "\n",
    "data_capture_prefix = \"{}/datacapture\".format(prefix)\n",
    "s3_capture_upload_path = \"s3://{}/{}\".format(bucket, data_capture_prefix)\n",
    "reports_prefix = \"{}/reports\".format(prefix)\n",
    "s3_report_path = \"s3://{}/{}\".format(bucket, reports_prefix)\n",
    "#code_prefix = \"{}/code\".format(prefix)\n",
    "#s3_code_preprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"preprocessor.py\")\n",
    "#s3_code_postprocessor_uri = \"s3://{}/{}/{}\".format(bucket, code_prefix, \"postprocessor.py\")\n",
    "\n",
    "ground_truth_upload_path = (\n",
    "    f\"s3://{bucket}/{prefix}/ground_truth_data/{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    ")\n",
    "\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Report path: {}\".format(s3_report_path))\n",
    "#print(\"Preproc Code path: {}\".format(s3_code_preprocessor_uri))\n",
    "#print(\"Postproc Code path: {}\".format(s3_code_postprocessor_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Setup service clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client(\"s3\")\n",
    "sagemaker_runtime_client = boto3.Session().client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Enable datacapture on a SageMaker endpoint \n",
    "\n",
    "Create an endpoint to showcase the data capture capability in action.\n",
    "\n",
    "For the endpoint we will use a pre-trained XGBoost model that is ready to deploy. This model was trained in the previous chapters using the weather dataset and has been included in the model directory for ease of use.\n",
    "\n",
    "Note that you can also train a new model and use your model and data below as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Upload the model object into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open(\"model/weather-prediction-model.tar.gz\", \"rb\")\n",
    "s3_key = os.path.join(prefix, \"weather-prediction-model.tar.gz\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(s3_key).upload_fileobj(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2  Create SageMaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name:  weather-pred-model-monitor-2021-08-04-1719\n",
      "https://bestpractices-bucket-sm.s3-us-west-2.amazonaws.com/FeatureAttributionMonitoring/weather-prediction-model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"weather-pred-model-monitor-{datetime.utcnow():%Y-%m-%d-%H%M}\"\n",
    "print(\"Model name: \", model_name)\n",
    "\n",
    "model_url = \"https://{}.s3-{}.amazonaws.com/{}/weather-prediction-model.tar.gz\".format(\n",
    "    bucket, region, prefix\n",
    ")\n",
    "\n",
    "print(model_url)\n",
    "\n",
    "image_uri = retrieve(\"xgboost\", boto3.Session().region_name, \"1.2-1\")\n",
    "\n",
    "model = Model(name=model_name, image_uri=image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value\n"
     ]
    }
   ],
   "source": [
    "##Test and validation files to use with model\n",
    "test_dataset=\"data/t_file.csv\"\n",
    "validation_dataset=\"data/v_file.csv\"\n",
    "dataset_type = \"text/csv\"\n",
    "\n",
    "with open(validation_dataset) as f:\n",
    "    headers_line = f.readline().rstrip()\n",
    "    all_headers = headers_line.split(\",\")\n",
    "##Get the label name\n",
    "label_header = all_headers[0]\n",
    "print(label_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3  Configure datacapture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture on the endpoint, you specify the new capture option called `DataCaptureConfig`. On enabling data capture, input to and output from the SageMaker endpoint are captured and saved in S3. Input captured includes the live inference traffic requests and output captured includes predictions from the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName=weather-prediction-fa-drift-model-monitor-2021-08-04-17-21-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: weather-pred-model-monitor-2021-08-04-1719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"weather-prediction-fa-drift-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, sampling_percentage=100, destination_s3_uri=s3_capture_upload_path\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Capture data from endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for about 3 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use the test file in the data directory  to execute inferences using the test file 't_file.csv' provided\n",
    "with open('data/t_file.csv', 'r') as TF:\n",
    "    t_lines = TF.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a method to run inferences against the endpoint\n",
    "def get_predictions():\n",
    "    smrt = boto3.Session().client(\"sagemaker-runtime\")\n",
    "    #Skip the first line since it has column headers\n",
    "    for tl in t_lines[1:50]:\n",
    "        #Remove the first column since it is the label\n",
    "        test_list = tl.split(\",\")\n",
    "        test_list.pop(0)\n",
    "        test_string = ','.join([str(elem) for elem in test_list])\n",
    "        \n",
    "        #print(\"invoking with payload \" + test_string)\n",
    "    \n",
    "        result = smrt.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType=\"text/csv\",\n",
    "                                   Body=test_string)\n",
    "        rbody = StreamingBody(raw_stream=result['Body'],content_length=int(result['ResponseMetadata']['HTTPHeaders']['content-length']))\n",
    "        #print(f\"Result from {result['InvokedProductionVariant']} = {rbody.read().decode('utf-8')}\")\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................."
     ]
    }
   ],
   "source": [
    "#Get predictions\n",
    "get_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5  View captured data\n",
    "\n",
    "Now list the data capture files stored in Amazon S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the Amazon S3 path is:\n",
    "\n",
    "`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://bestpractices-bucket-sm/FeatureAttributionMonitoring/datacapture'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_capture_upload_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Capture Files:\n",
      "FeatureAttributionMonitoring/datacapture/weather-prediction-fa-drift-model-monitor-2021-08-04-17-21-12/AllTraffic/2021/08/04/17/33-09-047-8f7d436a-6d6d-4314-bb08-fd3349a7ffaa.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Note : If you see an error in this cell, it could be because the captured files didn't appear in S3 yet.\n",
    "#Retry after a minute.\n",
    "current_endpoint_capture_prefix = \"{}/{}\".format(data_capture_prefix, endpoint_name)\n",
    "\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get(\"Contents\")]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, view the content of a single capture file. Take a quick peek at the first few lines in the captured file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"1585e9cd-a9c8-4d55-ab19-0f871d3cf094\",\"inferenceTime\":\"2021-08-04T17:33:09Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"4c980935-76a5-4070-ab82-7e8fb5967d97\",\"inferenceTime\":\"2021-08-04T17:33:09Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"5abad812-63f5-49ed-9931-9139c3c1a934\",\"inferenceTime\":\"2021-08-04T17:33:10Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetadata\":{\"eventId\":\"6f942304-00dc-4380-b77d-30b4332bc2c6\",\"inferenceTime\":\"2021-08-04T17:33:10Z\"},\"eventVersion\":\"0\"}\n",
      "{\"captureData\":{\"endpointInput\":{\"observedContentType\":\"text/csv\",\"mode\":\"INPUT\",\"data\":\"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\"encoding\":\"CSV\"},\"endpointOutput\":{\"observedContentType\":\"text/csv; charset=utf-8\",\"mode\":\"OUTPUT\",\"data\":\"-4.902510643005371\",\"encoding\":\"CSV\"}},\"eventMetad\n"
     ]
    }
   ],
   "source": [
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get(\"Body\").read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "capture_file = get_obj_body(capture_files[-1])\n",
    "print(capture_file[:2000])\n",
    "\n",
    "#capture_file = S3Downloader.read_file(capture_files[-1]).split(\"\\n\")[-10:-1]\n",
    "#print(capture_file[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the contents of a single line is present below in a formatted JSON file to observe a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"0,2020,12,4,31,0,19.0,0.0,6.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\\n\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"-4.902510643005371\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"1585e9cd-a9c8-4d55-ab19-0f871d3cf094\",\n",
      "    \"inferenceTime\": \"2021-08-04T17:33:09Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#print(json.dumps(json.loads(capture_file[-1]), indent=2))\n",
    "print(json.dumps(json.loads(capture_file.split(\"\\n\")[0]), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a baseline with ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start generating some artificial traffic\n",
    "The cell below starts a thread to send some traffic to the endpoint. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `inferenceId` attribute used above to invoke. If this is present, it will be used to join with ground truth data (otherwise `eventId` will be used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "import time\n",
    "\n",
    "#Invoke the endpoint in a loop\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        get_predictions()\n",
    "        \n",
    "# Note that you need to stop the kernel to stop the invocations\n",
    "#thread = Thread(target=invoke_endpoint_forever)\n",
    "#thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start generating some fake ground truth\n",
    "\n",
    "Besides captures, model bias monitoring execution also requires ground truth data. In real use cases, ground truth data should be regularly collected and uploaded to designated S3 location. In this example notebook, below code snippet is used to generate fake ground truth data. The first-party merge container will combine captures and ground truth data, and the merged data will be passed to model bias monitoring job for analysis. Similar to captures, the model bias monitoring execution will fail if there's no data to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_dataset_size  = 350\n",
    "\n",
    "\n",
    "def ground_truth_with_id(inference_id):\n",
    "    random.seed(inference_id)  # to get consistent results\n",
    "    rand = random.random()\n",
    "    # format required by the merge container\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"1\" if rand < 0.7 else \"0\",  # randomly generate positive labels 70% of the time # randomly generate positive labels 70% of the time #\n",
    "             # TODO : Need to make this a decimal??\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": str(inference_id),\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "\n",
    "def upload_ground_truth(upload_time):\n",
    "    records = [ground_truth_with_id(i) for i in range(test_dataset_size)]\n",
    "    fake_records = [json.dumps(r) for r in records]\n",
    "    data_to_upload = \"\\n\".join(fake_records)\n",
    "    target_s3_uri = f\"{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    print(f\"Uploading {len(fake_records)} records to\", target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 350 records to s3://bestpractices-bucket-sm/BiasDriftFeatureAttributionMonitoring/ground_truth_data/2021-08-02-23-38-12/2021/08/03/17/2451.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Generate data for the last hour\n",
    "upload_ground_truth(datetime.utcnow() - timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 350 records to s3://bestpractices-bucket-sm/BiasDriftFeatureAttributionMonitoring/ground_truth_data/2021-08-02-23-38-12/2021/08/03/18/2523.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Generate data once a hour\n",
    "#def generate_fake_ground_truth(terminate_event):\n",
    "def generate_fake_ground_truth_forever():\n",
    "    upload_ground_truth(datetime.utcnow())\n",
    "    for _ in range(0, 60):\n",
    "        time.sleep(60)\n",
    "        #if terminate_event.is_set():\n",
    "         #   break\n",
    "\n",
    "gt_thread = Thread(target=generate_fake_ground_truth_forever)\n",
    "gt_thread.start()\n",
    "\n",
    "#ground_truth_thread = WorkerThread(do_run=generate_fake_ground_truth)\n",
    "#ground_truth_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prefix = prefix + \"/baselining\"\n",
    "baseline_data_prefix = baseline_prefix + \"/data\"\n",
    "baseline_results_prefix = baseline_prefix + \"/results\"\n",
    "\n",
    "baseline_data_uri = f\"s3://{bucket}/{baseline_data_prefix}\"\n",
    "baseline_results_uri = f\"s3://{bucket}/{baseline_results_prefix}\"\n",
    "model_bias_baselining_job_result_uri = f\"{baseline_results_uri}/model_bias\"\n",
    "\n",
    "print(f\"Baseline data uri: {baseline_data_uri}\")\n",
    "print(f\"Baseline results uri: {baseline_results_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO : Delete it from here or from the previous section\n",
    "validation_dataset = \"v_file.csv\"\n",
    "\n",
    "validation_dataset = \"data/data-drift-baseline-data.csv\"  ##Lets try with this\n",
    "#dataset_type = \"text/csv\"\n",
    "\n",
    "#with open(validation_dataset) as f:\n",
    " #   headers_line = f.readline().rstrip()\n",
    "#all_headers = headers_line.split(\",\")\n",
    "#label_header = all_headers[0]\n",
    "\n",
    "#with open('data/v_file.csv', 'r') as TF:\n",
    " #   t_lines = TF.readlines()\n",
    "    \n",
    "#df_for_validation = pd.read_csv(\"data/v_file.csv\")\n",
    "#df_for_validation['city'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelConfig` is configuration related to model to be used for inferencing. In order to compute post-training bias metrics, the computation needs to get inferences for the model name provided. To accomplish this, the processing job will use the model to create an ephemeral endpoint (also known as \"shadow endpoint\"). The processing job will delete the shadow endpoint after the computations are completed. The configuration is also used by explainability monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_instance_count=1\n",
    "\n",
    "endpoint_instance_type=\"ml.m4.xlarge\"\n",
    "    \n",
    "model_config = ModelConfig(\n",
    "    model_name=model_name,\n",
    "    instance_count=endpoint_instance_count,\n",
    "    instance_type=endpoint_instance_type,\n",
    "    content_type=dataset_type,\n",
    "    accept_type=dataset_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Schedule continous monitoring\n",
    "When you have collected the data above, analyze and monitor the data with Monitoring Schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Generate prediction data for Model Quality  Monitoring\n",
    "\n",
    "Start generating some artificial traffic.  The cell below starts a thread to send some traffic to the endpoint. Note that you need to stop the kernel to terminate this thread. If there is no traffic, the monitoring jobs are marked as `Failed` since there is no data to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def ground_truth_with_id(inference_id):\n",
    "    random.seed(inference_id)  # to get consistent results\n",
    "    rand = random.random()\n",
    "    return {\n",
    "        \"groundTruthData\": {\n",
    "            \"data\": \"1\" if rand < 0.7 else \"0\",  # randomly generate positive labels 70% of the time #\n",
    "             # TODO : Need to make this a decimal??\n",
    "            \"encoding\": \"CSV\",\n",
    "        },\n",
    "        \"eventMetadata\": {\n",
    "            \"eventId\": str(inference_id),\n",
    "        },\n",
    "        \"eventVersion\": \"0\",\n",
    "    }\n",
    "\n",
    "\n",
    "def upload_ground_truth(records, upload_time):\n",
    "    fake_records = [json.dumps(r) for r in records]\n",
    "    data_to_upload = \"\\n\".join(fake_records)\n",
    "    target_s3_uri = f\"{ground_truth_upload_path}/{upload_time:%Y/%m/%d/%H/%M%S}.jsonl\"\n",
    "    print(f\"Uploading {len(fake_records)} records to\", target_s3_uri)\n",
    "    S3Uploader.upload_string_as_file_body(data_to_upload, target_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 300 records to s3://bestpractices-bucket-sm/BiasDriftFeatureAttributionMonitoring/ground_truth_data/2021-08-03-18-31-49/2021/08/04/15/3411.jsonl\n",
      "Uploading 300 records to s3://bestpractices-bucket-sm/BiasDriftFeatureAttributionMonitoring/ground_truth_data/2021-08-03-18-31-49/2021/08/04/15/5615.jsonl\n"
     ]
    }
   ],
   "source": [
    "NUM_GROUND_TRUTH_RECORDS = 300\n",
    "\n",
    "\n",
    "def generate_fake_ground_truth_forever():\n",
    "    j = 0\n",
    "    while True:\n",
    "        fake_records = [ground_truth_with_id(i) for i in range(NUM_GROUND_TRUTH_RECORDS)]\n",
    "        upload_ground_truth(fake_records, datetime.utcnow())\n",
    "        j = (j + 1) % 5\n",
    "        sleep(60 * 60)  # do this once an hour\n",
    "\n",
    "\n",
    "gt_thread = Thread(target=generate_fake_ground_truth_forever)\n",
    "gt_thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Create a monitoring schedule\n",
    "\n",
    "Now that you have the baseline information and ground truth labels, create a monitoring schedule to run model quality monitoring job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART C: Model Explainability Monitor\n",
    "\n",
    "Model explainability monitor can explain the predictions of a deployed model producing inferences and detect feature attribution drift on a regular basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "model_explainability_monitor = ModelExplainabilityMonitor(\n",
    "    role=role,\n",
    "    sagemaker_session=session,\n",
    "    #max_runtime_in_seconds=5400,\n",
    "    max_runtime_in_seconds=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baselining job\n",
    "\n",
    "Similary, a baselining job can be scheduled to suggest constraints for model explainability monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the explainability baselining job shares the test dataset with the bias baselining job, so here it uses the same `DataConfig`, the only difference is the job output URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_explainability_baselining_job_result_uri = f\"{baseline_results_uri}/model_explainability\"\n",
    "model_explainability_data_config = DataConfig(\n",
    "    s3_data_input_path=validation_dataset,\n",
    "    s3_output_path=model_explainability_baselining_job_result_uri,\n",
    "    label=label_header,\n",
    "    headers=all_headers,\n",
    "    dataset_type=dataset_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the Clarify explainer offers a scalable and efficient implementation of SHAP, so the explainability config is `SHAPConfig`, including\n",
    "* baseline: A list of rows (at least one) or S3 object URI to be used as the baseline dataset in the Kernel SHAP algorithm. The format should be the same as the dataset format. Each row should contain only the feature columns/values and omit the label column/values.\n",
    "* num_samples: Number of samples to be used in the Kernel SHAP algorithm. This number determines the size of the generated synthetic dataset to compute the SHAP values.\n",
    "* agg_method: Aggregation method for global SHAP values. Valid values are\n",
    "  * \"mean_abs\" (mean of absolute SHAP values for all instances),\n",
    "  * \"median\" (median of SHAP values for all instances) and\n",
    "  * \"mean_sq\" (mean of squared SHAP values for all instances).\n",
    "* use_logit: Indicator of whether the logit function is to be applied to the model predictions. Default is False. If \"use_logit\" is true then the SHAP values will have log-odds units.\n",
    "* save_local_shap_values (bool): Indicator of whether to save the local SHAP values in the output location. Default is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ismobile  year  month  quarter  day  isBadAir  location   city  \\\n",
      "0             0  2020     12        4   31         0      19.0    0.0   \n",
      "1             0  2020     12        4   31         0      19.0    0.0   \n",
      "2             0  2020     12        4   31         0      19.0    0.0   \n",
      "3             0  2020     12        4   31         0      19.0    0.0   \n",
      "4             0  2020     12        4   31         0      19.0    0.0   \n",
      "...         ...   ...    ...      ...  ...       ...       ...    ...   \n",
      "23154         0  2021      1        1    1         0    3424.0  127.0   \n",
      "23155         0  2020     12        4   31         0     333.0  165.0   \n",
      "23156         0  2020     12        4   31         0     333.0  165.0   \n",
      "23157         0  2020     12        4   31         0     333.0  165.0   \n",
      "23158         0  2020     12        4   31         0     333.0  165.0   \n",
      "\n",
      "       sourcename  sourcetype  no2   o3  pm10  pm25  so2   co  \n",
      "0             6.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "1             6.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "2             6.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "3             6.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "4             6.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "...           ...         ...  ...  ...   ...   ...  ...  ...  \n",
      "23154        13.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "23155         5.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "23156         5.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "23157         5.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "23158         5.0         0.0  0.0  0.0   0.0   0.0  0.0  1.0  \n",
      "\n",
      "[23159 rows x 16 columns]\n",
      "[[0.0, 2020.2729824258388, 8.876721792823524, 3.1487542640010364, 22.46064165119392, 0.08730946932078242, 1685.4437151863206, 366.1484951854571, 7.059328986571096, 0.0, 0.23731594628438188, 0.20812643032946154, 0.1703441426659182, 0.14236365991623126, 0.13722526879398939, 0.10432229370870935]]\n"
     ]
    }
   ],
   "source": [
    "# Here use the mean value of test dataset as SHAP baseline\n",
    "test_dataframe_full = pd.read_csv(test_dataset, header=[0])\n",
    "#Remove the first column since it is the label\n",
    "test_dataframe = test_dataframe_full.iloc[:, 1:]\n",
    "print(test_dataframe)\n",
    "\n",
    "shap_baseline = [list(test_dataframe.mean())]\n",
    "\n",
    "print(shap_baseline)\n",
    "\n",
    "shap_config = SHAPConfig(\n",
    "    baseline=shap_baseline,\n",
    "    num_samples=50,\n",
    "    #num_samples=100,\n",
    "    agg_method=\"mean_abs\",\n",
    "    save_local_shap_values=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kick off baselining job\n",
    "\n",
    "The same model_config is required, because the explainability baselining job needs to create shadow endpoint to get predictions for generated synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2021-08-04-17-39-09-269\n",
      "Inputs:  [{'InputName': 'dataset', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-802439482869/baseline-suggestion-job-2021-08-04-17-39-09-269/input/dataset/data-drift-baseline-data.csv', 'LocalPath': '/opt/ml/processing/input/data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'analysis_config', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://bestpractices-bucket-sm/FeatureAttributionMonitoring/baselining/results/model_explainability/analysis_config.json', 'LocalPath': '/opt/ml/processing/input/config', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'analysis_result', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://bestpractices-bucket-sm/FeatureAttributionMonitoring/baselining/results/model_explainability', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "ModelExplainabilityMonitor baselining job: baseline-suggestion-job-2021-08-04-17-39-09-269\n"
     ]
    }
   ],
   "source": [
    "model_explainability_monitor.suggest_baseline(\n",
    "    data_config=model_explainability_data_config,\n",
    "    model_config=model_config,\n",
    "    explainability_config=shap_config,\n",
    ")\n",
    "print(\n",
    "    f\"ModelExplainabilityMonitor baselining job: {model_explainability_monitor.latest_baselining_job_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for baselining job to finish (or skip this cell because the monitor to be scheduled will wait for it anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!"
     ]
    }
   ],
   "source": [
    "model_explainability_monitor.latest_baselining_job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspects the constraints suggested by the baseline job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelExplainabilityMonitor suggested constraints: s3://bestpractices-bucket-sm/FeatureAttributionMonitoring/baselining/results/model_explainability/analysis.json\n",
      "{\n",
      "    \"version\": \"1.0\",\n",
      "    \"explanations\": {\n",
      "        \"kernel_shap\": {\n",
      "            \"label0\": {\n",
      "                \"global_shap_values\": {\n",
      "                    \"ismobile\": 0.007424901047090124,\n",
      "                    \"year\": 0.009579587470671898,\n",
      "                    \"month\": 0.010584970117213112,\n",
      "                    \"quarter\": 0.007406574647169218,\n",
      "                    \"day\": 0.007720702139818704,\n",
      "                    \"isBadAir\": 0.011302626773941254,\n",
      "                    \"location\": 0.0192333616285298,\n",
      "                    \"city\": 0.01893646882046639,\n",
      "                    \"sourcename\": 0.01956434880904637,\n",
      "                    \"sourcetype\": 0.007430860663886335,\n",
      "                    \"no2\": 0.007457049793477212,\n",
      "                    \"o3\": 0.007441452425478423,\n",
      "                    \"pm10\": 0.014456348477081812,\n",
      "                    \"pm25\": 0.007253563406428856,\n",
      "                    \"so2\": 0.007315695852901553,\n",
      "                    \"co\": 0.036409936038425454\n",
      "                },\n",
      "                \"expected_value\": 0.17167794704437256\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_explainability_constraints = model_explainability_monitor.suggested_constraints()\n",
    "print()\n",
    "print(\n",
    "    f\"ModelExplainabilityMonitor suggested constraints: {model_explainability_constraints.file_s3_uri}\"\n",
    ")\n",
    "print(S3Downloader.read_file(model_explainability_constraints.file_s3_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule model explainability monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `create_monitoring_schedule()` method to schedule a hourly monitor, to analyze the data with monitoring schedule. If a baselining job has been submitted, then the monitor will automatically pick up analysis configuration from the baselining job. But if the baselining step is skipped, or the capture dataset has different nature than the training dataset, then analysis configuration has to be provided.\n",
    "\n",
    "`ModelConfig` is required by `ExplainabilityAnalysisConfig` for the same reason as it is required by the baselining job. Note that only features are required for computing feature attribution, so ground truth label should be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cron(0 * ? * * *)'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CronExpressionGenerator.hourly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cron(0 0/2 ? * * *)'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CronExpressionGenerator.daily_every_x_hours(hour_interval=2, starting_hour=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.model_monitor.clarify_model_monitoring.ClarifyBaseliningJob at 0x7fd629069208>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_explainability_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_explainability_analysis_config = None\n",
    "#if not model_explainability_monitor.latest_baselining_job:\n",
    "    # Remove label because only features are required for the analysis\n",
    " #   headers_without_label_header = copy.deepcopy(all_headers)\n",
    "  #  headers_without_label_header.remove(label_header)\n",
    "   # model_explainability_analysis_config = ExplainabilityAnalysisConfig(\n",
    "    #    explainability_config=shap_config,\n",
    "     #   model_config=model_config,\n",
    "      #  headers=headers_without_label_header,\n",
    "    #)\n",
    "    \n",
    "model_explainability_monitor.create_monitoring_schedule(\n",
    "    output_s3_uri=s3_report_path,\n",
    "    endpoint_input=endpoint_name,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly()\n",
    "    #schedule_cron_expression=schedule_expression,\n",
    "    #schedule_cron_expression=CronExpressionGenerator.daily_every_x_hours(hour_interval=2, starting_hour=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for execution and inspect analysis results\n",
    "\n",
    "Once created the schedule is started by default, here wait for the its first execution to start, then stop the schedule to avoid incurring charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_execution_to_start(model_monitor):\n",
    "    print(\n",
    "        \"A hourly schedule was created above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\"\n",
    "    )\n",
    "\n",
    "    print(\"Waiting for the first execution to happen\", end=\"\")\n",
    "    schedule_desc = model_monitor.describe_schedule()\n",
    "    while \"LastMonitoringExecutionSummary\" not in schedule_desc:\n",
    "        schedule_desc = model_monitor.describe_schedule()\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(60)\n",
    "    print()\n",
    "    print(\"Done! Execution has been created\")\n",
    "\n",
    "    print(\"Now waiting for execution to start\", end=\"\")\n",
    "    while schedule_desc[\"LastMonitoringExecutionSummary\"][\"MonitoringExecutionStatus\"] in \"Pending\":\n",
    "        schedule_desc = model_monitor.describe_schedule()\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print()\n",
    "    print(\"Done! Execution has started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hourly schedule was created above and it will kick off executions ON the hour (plus 0 - 20 min buffer).\n",
      "Waiting for the first execution to happen.........................................\n",
      "Done! Execution has been created\n",
      "Now waiting for execution to start\n",
      "Done! Execution has started\n"
     ]
    }
   ],
   "source": [
    "wait_for_execution_to_start(model_explainability_monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_explainability_monitor.stop_monitoring_schedule()\n",
    "#model_explainability_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait further for the execution to finish, then inspect its analysis results,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waits for the schedule to have last execution in a terminal status.\n",
    "def wait_for_execution_to_finish(model_monitor):\n",
    "    schedule_desc = model_monitor.describe_schedule()\n",
    "    execution_summary = schedule_desc.get(\"LastMonitoringExecutionSummary\")\n",
    "    if execution_summary is not None:\n",
    "        print(\"Waiting for execution to finish\", end=\"\")\n",
    "        while execution_summary[\"MonitoringExecutionStatus\"] not in [\n",
    "            \"Completed\",\n",
    "            \"CompletedWithViolations\",\n",
    "            \"Failed\",\n",
    "            \"Stopped\",\n",
    "        ]:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(60)\n",
    "            schedule_desc = model_monitor.describe_schedule()\n",
    "            execution_summary = schedule_desc[\"LastMonitoringExecutionSummary\"]\n",
    "        print()\n",
    "        print(\"Done! Execution has finished\")\n",
    "    else:\n",
    "        print(\"Last execution not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for execution to finish\n",
      "Done! Execution has finished\n"
     ]
    }
   ],
   "source": [
    "wait_for_execution_to_finish(model_explainability_monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====STOP==== \n",
      " No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\n"
     ]
    }
   ],
   "source": [
    "schedule_desc = model_explainability_monitor.describe_schedule()\n",
    "execution_summary = schedule_desc.get(\"LastMonitoringExecutionSummary\")\n",
    "if execution_summary and execution_summary[\"MonitoringExecutionStatus\"] in [\n",
    "    \"Completed\",\n",
    "    \"CompletedWithViolations\",\n",
    "]:\n",
    "    last_model_explainability_monitor_execution = model_explainability_monitor.list_executions()[-1]\n",
    "    last_model_explainability_monitor_execution_report_uri = (\n",
    "        last_model_explainability_monitor_execution.output.destination\n",
    "    )\n",
    "    print(f\"Report URI: {last_model_explainability_monitor_execution_report_uri}\")\n",
    "    last_model_explainability_monitor_execution_report_files = sorted(\n",
    "        S3Downloader.list(last_model_explainability_monitor_execution_report_uri)\n",
    "    )\n",
    "    print(\"Found Report Files:\")\n",
    "    print(\"\\n \".join(last_model_explainability_monitor_execution_report_files))\n",
    "else:\n",
    "    last_model_explainability_monitor_execution = None\n",
    "    print(\n",
    "        \"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, they will be listed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if last_model_explainability_monitor_execution:\n",
    "    model_explainability_violations = (\n",
    "        last_model_explainability_monitor_execution.constraint_violations()\n",
    "    )\n",
    "    if model_explainability_violations:\n",
    "        print(model_explainability_violations.body_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis results and CloudWatch metrics are visualized in SageMaker Studio. Select the Endpoints tab, then double click the endpoint to show the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART D: Cleanup\n",
    "\n",
    "The endpoint can keep running and capturing data, but if there is no plan to collect more data or use this endpoint further, it should be deleted to avoid incurring additional charges. Note that deleting endpoint does not delete the data that was captured during the model invocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First stop the worker threads,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke_endpoint_thread.terminate()\n",
    "#ground_truth_thread.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then stop all monitors scheduled for the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: monitoring-schedule-2021-08-03-20-57-17-238\n"
     ]
    }
   ],
   "source": [
    "model_bias_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sagemaker_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-86c4df913bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel_monitors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_monitors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_monitor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_monitors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sagemaker_session' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "predictor = Predictor(endpoint_name, sagemaker_session=sagemaker_session)\n",
    "model_monitors = predictor.list_monitors()\n",
    "for model_monitor in model_monitors:\n",
    "    model_monitor.stop_monitoring_schedule()\n",
    "    wait_for_execution_to_finish(model_monitor)\n",
    "    model_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
